{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "8d6ab33286f78d080320ea4280550503030699c9878852cc65d630c9e03cc862"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(raw_csv):\n",
    "    # Read in csv and drop na\n",
    "    csv = pd.read_csv(raw_csv,infer_datetime_format=True)\n",
    "    csv = csv.dropna()\n",
    "    # Create mask from beginning to end of analyzed period\n",
    "    # Citation: https://www.interviewqs.com/ddi_code_snippets/select_pandas_dataframe_rows_between_two_dates\n",
    "    csv['timestamp'] = pd.to_datetime(csv['timestamp'])\n",
    "    start_date = '01-01-20 00:00:01'\n",
    "    end_date = '08-10-20 23:59:59'\n",
    "    mask = (csv['timestamp'] > start_date) & (csv['timestamp'] <= end_date)\n",
    "    csv = csv[mask]\n",
    "\n",
    "    # # If you run into trouble plotting with the time in the timestamps, uncommenting this will return only the date entries\n",
    "    # # We can decide whether we want to keep the first or last of the duplicates depending on how our data shapes out\n",
    "    # # Citation: https://stackoverflow.com/questions/35595710/splitting-timestamp-column-into-separate-date-and-time-columns\n",
    "    # csv['date'] = [d.date() for d in csv['timestamp']]\n",
    "    # csv['time'] = [d.time() for d in csv['timestamp']]\n",
    "    # csv = csv.drop(columns=['timestamp','time'])\n",
    "\n",
    "    new_csv = raw_csv.replace(\".csv\", \"_csv\")\n",
    "    new_csv = new_csv.replace(\"../data/rawdata\",\"\")\n",
    "    csv.to_csv(f\"../data/cleandata/{new_csv}_clean.csv\", index=False)\n",
    "    return csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    timestamp       Open       High        Low  Close  Adj Close   Volume\n0  2019-10-30  21.049999  21.180000  20.889999  21.15  20.251532  1583000\n1  2019-10-31  21.049999  21.049999  20.860001  20.99  20.098328  1533400\n2  2019-11-01  20.969999  21.170000  20.969999  21.17  20.270681  1242700\n3  2019-11-04  21.420000  21.580000  21.420000  21.58  20.663265  1419900\n4  2019-11-05  21.459999  21.580000  21.389999  21.58  20.663265  3102700\n    timestamp       Open       High        Low      Close  Adj Close    Volume\n0  2019-10-30  32.930000  33.340000  32.029999  33.130001  33.130001  78228800\n1  2019-10-31  32.980000  34.340000  32.820000  33.930000  33.930000  67881600\n2  2019-11-01  34.369999  35.000000  34.099998  34.889999  34.889999  64238600\n3  2019-11-04  35.189999  36.450001  34.759998  36.290001  36.290001  83343800\n4  2019-11-05  36.560001  37.180000  35.980000  36.150002  36.150002  84848600\n    timestamp  Open   High   Low  Close  Adj Close  Volume\n0  2019-10-30  1.61  1.665  1.52   1.59       1.59  708800\n1  2019-10-31  1.63  1.630  1.52   1.56       1.56  322800\n2  2019-11-01  1.55  1.590  1.53   1.57       1.57  507100\n3  2019-11-04  1.57  1.670  1.55   1.66       1.66  799900\n4  2019-11-05  1.66  1.660  1.57   1.60       1.60  643500\n    timestamp       Open       High        Low      Close  Adj Close    Volume\n0  2019-10-30  62.599998  63.757999  61.993999  63.001999  63.001999  48209000\n1  2019-10-31  62.619999  63.799999  62.599998  62.984001  62.984001  25335000\n2  2019-11-01  63.264000  63.296001  61.959999  62.661999  62.661999  31919500\n3  2019-11-04  62.959999  64.388000  61.852001  63.493999  63.493999  43935000\n4  2019-11-05  63.924000  64.702003  63.223999  63.444000  63.444000  34717000\n"
     ]
    }
   ],
   "source": [
    "# Prepare Yahoo files to run through celaning function\n",
    "for yahoo_file in os.listdir('../data/rawdata/Yahoo_Finance_Data/'):\n",
    "    yahoo_csv = pd.read_csv(f'../data/rawdata/Yahoo_Finance_Data/{yahoo_file}', infer_datetime_format=True)\n",
    "    yahoo_csv.rename(columns={'Date' : 'timestamp'}, inplace=True)\n",
    "    yahoo_csv = yahoo_csv.loc[:,['timestamp', 'Open','High','Low','Close','Adj Close','Volume']]\n",
    "\n",
    "# Citation: https://pandasninja.com/2019/04/how-to-read-lots-of-csv-files-easily-into-pandas/\n",
    "raw_folders = os.listdir('../data/rawdata')\n",
    "for folder in raw_folders:\n",
    "    raw_csvs = os.listdir(f'../data/rawdata/{folder}')\n",
    "    for raw_csv in raw_csvs:\n",
    "        raw_csv = f'../data/rawdata/{folder}/{raw_csv}'\n",
    "        clean_data(raw_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}